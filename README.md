#  Local RAG Search System with Configurable YAML Pipeline

## Introduction
This project implements a **fully local Retrieval-Augmented Generation (RAG) system** that allows users to query company-related information using documents stored on their machine.  
It works without cloud dependencies and ensures **data privacy**, making it ideal for enterprise environments.

The system:
- Loads documents from a folder
- Splits and embeds them
- Stores embeddings in a FAISS vector index
- Retrieves relevant context
- Generates answers using a **local LLM via Ollama**

# Key Features
âœ… 100% local execution (no external APIs)  
âœ… Multi-format document ingestion:
- PDF, DOCX, TXT, CSV, JSON, Excel  
âœ… Configurable embeddings and chunking  
âœ… FAISS-based similarity search  
âœ… Local LLM inference via Ollama  
âœ… YAML-driven settings (models, paths, prompts, topics)  
âœ… Modular and extendable architecture  

# System Architecture

Documents â†’ Chunking â†’ Embeddings â†’ FAISS Index â†’ Retrieval â†’ Prompt Filling â†’ LLM Response


# Components
| Component | Responsibility |
|----------|----------------|
| `data_loader.py` | Reads and loads supported file types |
| `embedding.py` | Splits text and generates embeddings |
| `vectorstore.py` | Stores and queries embeddings using FAISS |
| `local_llm.py` | Loads and manages local LLM |
| `rag_search.py` | Performs retrieval + generation workflow |
| `config.yaml` | Centralized settings |

# Project Structure
project/
â”‚
â”œâ”€ src/
â”‚ â”œâ”€ components/
â”‚ â”‚ â”œâ”€ data_loader.py
â”‚ â”‚ â”œâ”€ embedding.py
â”‚ â”‚ â”œâ”€ vectorstore.py
â”‚ â”‚ â”œâ”€ local_llm.py
â”‚ â”‚
â”‚ â”œâ”€ rag_search.py
â”‚
â”œâ”€ data/ # Place your documents here
â”œâ”€ faiss_store/ # Auto-generated vector index
â”œâ”€ config.yaml # Configuration file
â”œâ”€ requirements.txt
â”œâ”€ README.md

Installation & Setup
1. Clone the repository
git clone <your-repo-url>
cd project
2. Install dependencies
pip install -r requirements.txt
3. Install & start Ollama
ollama pull mistral
4. Add documents
5. Run a sample query

ğŸ¤– How It Works Internally
1ï¸âƒ£ Document Loading

Scans folder and loads supported formats

2ï¸âƒ£ Chunking

Documents are broken into overlapping text blocks

3ï¸âƒ£ Embedding

Chunks converted into dense vectors using SentenceTransformers

4ï¸âƒ£ FAISS Indexing

Vectors stored for fast similarity lookup

5ï¸âƒ£ Querying

User question embedded and compared to stored vectors

6ï¸âƒ£ Prompt Construction

Template injected with:

context

company name

allowed topics

7ï¸âƒ£ Local LLM Generates Answer

Based only on allowed topics and given rules

Author

Pranjal Singh
AI | RAG | Automation Systems